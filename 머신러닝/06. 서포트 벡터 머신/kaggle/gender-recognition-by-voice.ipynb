{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/voicegender/voice.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/voicegender/voice.csv\")\ndf.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n\n          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n\n     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>...</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059781</td>\n      <td>0.064241</td>\n      <td>0.032027</td>\n      <td>0.015071</td>\n      <td>0.090193</td>\n      <td>0.075122</td>\n      <td>12.863462</td>\n      <td>274.402906</td>\n      <td>0.893369</td>\n      <td>0.491918</td>\n      <td>...</td>\n      <td>0.059781</td>\n      <td>0.084279</td>\n      <td>0.015702</td>\n      <td>0.275862</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066009</td>\n      <td>0.067310</td>\n      <td>0.040229</td>\n      <td>0.019414</td>\n      <td>0.092666</td>\n      <td>0.073252</td>\n      <td>22.423285</td>\n      <td>634.613855</td>\n      <td>0.892193</td>\n      <td>0.513724</td>\n      <td>...</td>\n      <td>0.066009</td>\n      <td>0.107937</td>\n      <td>0.015826</td>\n      <td>0.250000</td>\n      <td>0.009014</td>\n      <td>0.007812</td>\n      <td>0.054688</td>\n      <td>0.046875</td>\n      <td>0.052632</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.077316</td>\n      <td>0.083829</td>\n      <td>0.036718</td>\n      <td>0.008701</td>\n      <td>0.131908</td>\n      <td>0.123207</td>\n      <td>30.757155</td>\n      <td>1024.927705</td>\n      <td>0.846389</td>\n      <td>0.478905</td>\n      <td>...</td>\n      <td>0.077316</td>\n      <td>0.098706</td>\n      <td>0.015656</td>\n      <td>0.271186</td>\n      <td>0.007990</td>\n      <td>0.007812</td>\n      <td>0.015625</td>\n      <td>0.007812</td>\n      <td>0.046512</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.151228</td>\n      <td>0.072111</td>\n      <td>0.158011</td>\n      <td>0.096582</td>\n      <td>0.207955</td>\n      <td>0.111374</td>\n      <td>1.232831</td>\n      <td>4.177296</td>\n      <td>0.963322</td>\n      <td>0.727232</td>\n      <td>...</td>\n      <td>0.151228</td>\n      <td>0.088965</td>\n      <td>0.017798</td>\n      <td>0.250000</td>\n      <td>0.201497</td>\n      <td>0.007812</td>\n      <td>0.562500</td>\n      <td>0.554688</td>\n      <td>0.247119</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135120</td>\n      <td>0.079146</td>\n      <td>0.124656</td>\n      <td>0.078720</td>\n      <td>0.206045</td>\n      <td>0.127325</td>\n      <td>1.101174</td>\n      <td>4.333713</td>\n      <td>0.971955</td>\n      <td>0.783568</td>\n      <td>...</td>\n      <td>0.135120</td>\n      <td>0.106398</td>\n      <td>0.016931</td>\n      <td>0.266667</td>\n      <td>0.712812</td>\n      <td>0.007812</td>\n      <td>5.484375</td>\n      <td>5.476562</td>\n      <td>0.208274</td>\n      <td>male</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"meanfreq    0\nsd          0\nmedian      0\nQ25         0\nQ75         0\nIQR         0\nskew        0\nkurt        0\nsp.ent      0\nsfm         0\nmode        0\ncentroid    0\nmeanfun     0\nminfun      0\nmaxfun      0\nmeandom     0\nmindom      0\nmaxdom      0\ndfrange     0\nmodindx     0\nlabel       0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"결측치 없음"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.iloc[:,:-1]#마지막 열(label)만 제외한 부분\ny=df.iloc[:,-1]\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n#라벨 인코더 생성\nencoder =LabelEncoder()\n#x를 이용피팅하고 라벨숫자로 변환한다.\ny=encoder.fit_transform(y)\ny","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([1, 1, 1, ..., 0, 0, 0])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"y.head()로 적으면 오류남. dataframe이 아니라 array라서 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 표준화 작업\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nx_data = scaler.transform(x)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n# df의 20퍼센트를 test set으로 선택","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. SVM 모델 적용 - kernel 비교\n### 1) default hyperparameter을 이용하여 SVM 돌리기"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import metrics\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Score :')\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":9,"outputs":[{"output_type":"stream","text":"Accuracy Score :\n0.6829652996845426\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 2) linear kernel을 이용해 SVM돌리기"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Score :')\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":10,"outputs":[{"output_type":"stream","text":"Accuracy Score :\n0.9148264984227129\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 3) rbf"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='rbf')\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Score :')\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":11,"outputs":[{"output_type":"stream","text":"Accuracy Score :\n0.6829652996845426\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"    "},{"metadata":{},"cell_type":"markdown","source":"### 4) poly"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel='poly')\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Score :')\nprint(metrics.accuracy_score(y_test,y_pred))\n# 정확도 하락 (polynomial kernel은 training dataset을 overfitting해서 일 가능성 존재)","execution_count":12,"outputs":[{"output_type":"stream","text":"Accuracy Score :\n0.5031545741324921\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"linear방식의 accurcy가 제일 높음"},{"metadata":{},"cell_type":"markdown","source":" # 3. c값 조정\n ### 앞서 정확도가 제일 높게 나온 linear kernel 방식을 이용"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = [1, 10, 100, 1000]\nfor c in C :\n    svc = SVC(kernel='linear',C=c)\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    print('when C=',c,', Accuracy Score : ')\n    print(metrics.accuracy_score(y_test,y_pred))","execution_count":14,"outputs":[{"output_type":"stream","text":"when C= 1 , Accuracy Score : \n0.9148264984227129\nwhen C= 10 , Accuracy Score : \n0.9605678233438486\nwhen C= 100 , Accuracy Score : \n0.9637223974763407\nwhen C= 1000 , Accuracy Score : \n0.9542586750788643\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"C가 100일때 정확도가 가장 높게 나옴"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}